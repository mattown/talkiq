___________      .__   __     .___________
\__    ___/____  |  | |  | __ |   \_____  \
  |    |  \__  \ |  | |  |/ / |   |/  / \  \
  |    |   / __ \|  |_|    <  |   /   \_/.  \
  |____|  (____  /____/__|_ \ |___\_____\ \_/
               \/          \/            \__>

Coding challenge: https://gist.github.com/talkiq


#
# Important Note:
#

In the assignment it states that for solution to the moving median in feature2.txt is:

22.0
32.0
24.0

I think there is a typo because I think this is false, as per my own investigation and as stated in the document
the unique counts for the samples are

22 sample 1
42 sample 2
34 sample 3

The moving median should be
sample 1 [22]
22
sample 1 + 2 [22,42]

32 ( 22+ 42 /2)

sample 1+ 2 + 3 [22,34,42] (sorted to highlight the median!)

34 (since 34 is in the middle value of 22 and 42)

I believe the correct answer is

22
32
34


#
#  Install Notes
#

python3 is required

$ sudo apt-get install python3


#
#  Description
#
#

This simple etl script will take a target directory and recursively search for any .txt files in it, it will produce a sorted word count of unique words in the files, as well as the moving median of unique words in each file sorted by filename

It also assumes that there is an optional file named stopwords.txt in the same runtime directory and all this is is a list of specific words to filter out

The etl script stores the unique words as a simple hash index that gets updated as more files are read, everything is stored in memory before writing to disk

Another important note, this script assumes we read things as UTF-8 as it makes use of the special regex utf-8 function to parse ALL WHITESPACE that exists (think all languages) and checks for non-whitespace characters inbetween them, this is much better than just parsing on the default "space" value on North American language settings ; )

I realize this isn't the fastest possible way to do this, but given the time restraint I didn't have time to implement a faster solution, but if I were designing for performance, I would tweak this to ensure that we stream + thread multiple input files and also stream our moving median output


#
#  Run time
#
#

To run the script run the following

    $ python3 etl.py <target directory>

This will produce 2 files

    feature1.txt --> this has a sorted word count of unique words found in the target directory (excluding words found in stopwords.txt)

    feature2.txt --> total moving median unique word count by file read (sorted asc, excludes words found in stopwords.txt


For example for the folder with our sample data (test_data in this repo)

    cd to directory

    $ python3 etl.py test_data
